{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk PyPDF2 pdfplumber imbalanced-learn spacy xgboost\n!python -m spacy download en_core_web_sm\n\nimport os\nimport PyPDF2\nimport pdfplumber\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom nltk.corpus import stopwords\nimport re\nimport nltk\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\n\n# Download stopwords if needed\nnltk.download('stopwords')\n\n# Load spaCy model for NER and text processing\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Define stopwords\nSTOPWORDS = set(stopwords.words('english'))\n\n# Step 1: Extract text from PDF resumes\ndef extract_text_from_pdf(file_path):\n    try:\n        text = \"\"\n        with open(file_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            for page in reader.pages:\n                page_text = page.extract_text()\n                if page_text:\n                    text += page_text\n        return text\n    except:\n        with pdfplumber.open(file_path) as pdf:\n            text = \"\"\n            for page in pdf.pages:\n                page_text = page.extract_text()\n                if page_text:\n                    text += page_text\n        return text\n\n# Step 2: Preprocess text (cleaning and tokenization)\ndef clean_text(text):\n    text = text.lower()  # Lowercase text\n    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])  # Remove stopwords\n    return text\n\n# Step 4: Load and preprocess dataset of resumes from multiple folders\ndef load_resumes_from_folders(base_folder):\n    resumes = []\n    labels = []\n    \n    # Walk through all folders and files\n    for root, _, files in os.walk(base_folder):\n        job_title = os.path.basename(root)  # Folder name is the job title\n        print(f\"Processing folder: {root}\")  # Check folder path\n        for file in files:\n            if file.endswith(\".pdf\"):\n                file_path = os.path.join(root, file)\n                print(f\"Processing resume: {file_path}\")  # Print file being processed\n                resume_text = extract_text_from_pdf(file_path)\n                \n                if resume_text.strip():  # Ensure resume is not empty\n                    resume_cleaned = clean_text(resume_text)\n                    resumes.append(resume_cleaned)\n                    labels.append(job_title)  # Label based on folder name\n    \n    print(f\"\\nTotal Resumes Processed: {len(resumes)}\")\n    return resumes, labels\n\n# Step 5: Train and save the model using XGBoost\ndef train_and_save_model(base_folder, model_path, vectorizer_path):\n    # Load resumes and their corresponding job titles\n    resumes, labels = load_resumes_from_folders(base_folder)\n    \n    # Split the data into training and test sets (80% train, 20% test)\n    X_train, X_test, y_train, y_test = train_test_split(resumes, labels, test_size=0.2, random_state=42)\n    \n    # Convert resume text to TF-IDF features\n    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))  # Increased features, bi-grams\n    X_train_tfidf = vectorizer.fit_transform(X_train)\n    \n    # Use SMOTE to oversample the minority classes\n    smote = SMOTE(random_state=42)\n    X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n    \n    # Define the parameter grid for tuning\n    param_grid = {\n        'n_estimators': [200, 300, 400],\n        'max_depth': [10, 15, 20],\n        'learning_rate': [0.1, 0.05, 0.01],\n        'colsample_bytree': [0.7, 0.8, 0.9],\n        'subsample': [0.7, 0.8, 0.9]\n    }\n\n    # Initialize the XGBoost classifier\n    xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n\n    # Perform Grid Search\n    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=-1)\n    grid_search.fit(X_train_smote, y_train_smote)\n\n    # Use the best model from the grid search\n    best_xgb = grid_search.best_estimator_\n\n    # Print the best parameters and accuracy\n    print(f\"Best Parameters: {grid_search.best_params_}\")\n    print(f\"Best Accuracy from CV: {grid_search.best_score_:.2f}\")\n\n    # Evaluate the best model on the test set\n    X_test_tfidf = vectorizer.transform(X_test)\n    y_pred = best_xgb.predict(X_test_tfidf)\n\n    # Display metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"\\nFinal Test Accuracy: {accuracy * 100:.2f}%\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred))\n    \n    # Confusion Matrix\n    print(\"\\nConfusion Matrix:\")\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=set(y_test), yticklabels=set(y_test))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    # Save the best trained model and vectorizer\n    joblib.dump(best_xgb, model_path)\n    joblib.dump(vectorizer, vectorizer_path)\n    \n    print(f\"Model saved to {model_path}\")\n    print(f\"Vectorizer saved to {vectorizer_path}\")\n# Main execution for training and saving the model\nif __name__ == \"__main__\":\n    base_folder = \"/kaggle/input/resume-dataset/data/data\"  # Replace with the path to your dataset\n    model_path = \"xgb_resume_classifier_model.pkl\"\n    vectorizer_path = \"tfidf_vectorizer.pkl\"\n    \n    # Train and save the model\n    train_and_save_model(base_folder, model_path, vectorizer_path)\n\n    print(\"\\nModel training and saving completed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T11:01:55.732878Z","iopub.execute_input":"2024-09-22T11:01:55.733344Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","output_type":"stream"}]}]}